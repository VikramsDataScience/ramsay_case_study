# Ramsay Case Study

## Part 1: Data Exploration and Preparation
### On data issues:
<b>N.B For the data issues identified here, please refer to the `src.preprocessing` module for the solutions.</b><br><br>
Referring to the ydata profiling report to help with EDA (please refer to `src.eda_profile_report`), and the identification of any quality/data issues, the following has come up:
- Missing numerical values in `CCU_Charges`, `ICU_Charge`, `TheatreCharge`, `ProsthesisCharge`, `OtherCharges`, `BundledCharges`, `UnplannedTheatreVisit`, `InfantWeight`, `Readmission28Days`, `HoursMechVentilation`, `PalliativeCareStatus`. The solution to this would be dynamic imputation using the <a href='https://pypi.org/project/MissForest/'> Missing Forest algorithm</a>. Missing Forest is more computationally expensive (especially for big datasets) but the result is a more dynamic imputation method for missing values. This is opposed to using simple means and medians, as any possible skewed values that may exist in the data set will only compound to become more skewed and further bias any inherent biases that may exist.
- Missing values that are mixed in `PharmacyCharge`. By mixed, there are many missing numerical values, but there are also some string  values such as 'error' that make up roughly 6.8% of the column. Since ~93% of the data in this column contain numerical values, I've chosen to convert the 'error' entries into NaNs.
<br>&nbsp;&nbsp;There is a further issue that expresses large integers in scientific notation (i.e. 1.09E+19). To address this issue required writing some fairly gnarly code in the `src.preprocessing` module. Specifically, the `clean_clinical_charges()` function addresses these issues. Firstly it required handling the 'e' aspect of the string dtype, separating the 'e' from the numerical value in the row and keeping the float value in the row. I think there were also some negative values that were handled by an 'if' clause. The other thing I did do, was set an upper limit/ceiling on the values that could be passed. Since many of the numbers were extremely large (far too large for any reasonble DRG to charge patients, unless they were Jeff Bezos!), I elected to cap off the numerical figures at 1000, and also perform a conversion from cents to dollars by $\times100$ - I'm not sure if this was the right thing to do, but when I reviewed the values in the resulting CSV file, it _looked_ correct (but I'm happy to be told that I was wrong in how I handled it!).
<br>&nbsp;&nbsp;From here, I elected to use the Missing Forest algorithm to perform dynamic imputation.
- Missing values in `UnplannedTheatreVisit`, `Readmission28Days`, and `PalliativeCareStatus` are binary categoricals (True/False). However, all three of these columns are missing ~96% of values! Since there are so many missing values in these categoricals, I've opted to create and impute a 3rd category called 'unknown'.
- Unsupported dtypes for `AdmissionTime` and `SeparationTime`. Python reads them as objects, and not the time as a 24 hour clock (HH:MM:SS). My proposed solution was to use pandas `to_datetime().dt.time` to only extract the values as time objects.

### On Feature creation:
<b>N.B. For the feature creation, please refer to the `src.preprocessing` module for the solution.</b><br>
- Since we have the `AdmissionDate` and the `SeparationDate` as part of the dataset, my thoughts were to create a `LengthOfStay` feature. Since patient LOS impacts private healthcare profitability and operations, there is a point of optimality to patient LOS, whilst still balancing both ethical and legal commitments to patient care. My thought was to create this feature and use it as a predictor for optimal LOS (please refer to the final section on Model Development for how I think this might be achieved).

## Part 2: Data Analysis and Visualisation
### DRG analysis
<b>N.B. For this section please refer to the `src.drg_analysis` module and plots generated by this module title `max_Charges_by_DRG.html` and `sum_Charges_by_DRG.html`.</b>
<br>&nbsp;&nbsp;For DRGs, I've looked and plotted with Plotly the DRGs that generate the Maximum revenue, the Mean, and the total Sum over the full course of the dataset:
- <b>Maximum Charges by DRG</b>: DRG10B with $14.18k in revenue, but only accounts for 20 surgeries in the total dataset. This is followed closely by DRG003 with $14.14k. A key difference is that DRG003 has generated 5,044 surgeries in the time period covered by the dataset!
- <b>Total (Summed) Charges by DRG</b>: DRG003, DRG002, and DRG001 are the largest 'earners' by way of total revenue, with total earnings of $37.73M, $37.66M, and $36.64M, respectively. Out of all the total DRGs these three combined account for about half of the dataset with a total of 14,987 surgeries across Urgent/Emergency, and Elective categories of admission.
<br>&nbsp;&nbsp;Certain DRGs, especially complex or high-risk treatments (e.g., involving ICU or prostheses), are likely to accrue the largest charges. Drivers of higher charges could be factors like CareType (inpatient/outpatient), emergency vs elective admissions, and usage of resources such as ICU and theatre facilities.

### Results of SQL query for total and average monthly admissions
<b>N.B. For this section please refer to the `src.sql_queries` module. And, for the results of those queries, please refer to `data/Total_Average_Admissions.csv` and `data/Percentile_Charges_by_Diagnosis_Sex.csv`.</b>
<br>&nbsp;&nbsp;Examining the results contained in the `Percentile_Charges_by_Diagnosis_Sex.csv` file, these are some observations:
- Some clinical codes don't change but are fixed charges. Initially I had thought my SQL query was very flawed! But closer examination then appeared that it may be that certain clinical codes don't vary in charges, but that was also not the case - one example is how A00.4 in row 4 does vary across percentile, but the same code doesn't vary in row 5, since it's a different patient (episode_id) that have different diagnoses (diagnosis1 -> 4 columns). These additional diagnoses (and therefore additional charges) could be driven by the presence of comorbidities or additional postsurgical complications? So in noting this it appears to be part of the data, and not the SQL query.
- In observing the above, I ran a filter to determine how often the distribution between the `25th Percentile` was the same as the `Maximum`. Out of the total 22,857 rows 16,957 (74%) of the cases the charges don't change. So, it looks like only in 26% of the surgical cases have the costs changed between the 25th Percentile and the Maximum.

## Part 3: Strategic Insights and Recommendations
Two possible evidence based insights:
- For the new feature I had created on Length of Stay, I did notice that about ~77% of the episode ids in the dataset have an LOS that is $\geq$ 7 days. Depending on the Case-Mix/Per Diem agreements with the Private Health Insurers (or out-of-pocket for those patients who're paying directly), I would think that the longer LOS would present an operations problem for Ramsay? From my Healthscope experience, I've come to understand the longer an LOS, the less profitable, and higher risk it is for the hospital, since it increases the likelihood of HACs (Hospital Acquired Complications) occuring? These HACs could either be a result of postsurgical complications or falls within the hospital. On this note, the final section on model development looks at a proposal for a possible model that could optimise LOS.
- The aforementioned DRG001, DRG002, and DRG003 were identified as the largest revenue generators in the dataset with a summed total of about $112M in revenue. From an acquisitions point of view, if this is indeed representative of Ramsay's revenue generation, it would make sense to focus on promoting Ramsay's hospitals to the VMOs (Visiting Medical Officers) either whose specialisations lie in performing those surgeries or, if its feasible, optimise some hospital Operating Theatre's such that they make performing such surgeries easier, safer for the patient, and/or more efficient? 
<br>&nbsp;&nbsp;Also, an additional possibility (if it's applicable, of course!) could lie in the purchase of surgical robots (if Ramsay doesn't already have them) that could assist in performing those surgeries more easily? A further drawcard could also lie in sharing some of the data that the surgical robots have with some of the surgeons who're performing research in those specialisations? From my Healthscope experience, I've been told by one of the leaders that sharing some of this data can sometimes be very alluring to researching VMOs.

## Part 4: Model Development
From a Data Science perspective, I feel that the first insight (alluding to the new feature, `LengthOfStay`) is probably the most actionable in terms of developing a model:
### Model purpose
- To minimise the cost and risks (given the aforementioned HACs that may arise the longer a patient remains admitted) to the business associated with extended Lengths of Stay(LOS).
### Model choice
- To me, minimisation of Length of Stay is a good candidate for Mathematical Optimisation. This is whereby, we assume LOS is the objective function that we're trying to minimise given certain Decision Variables (that which correspond to actions or choices that we can make in our decision problem) and Constraints (that which restrict the possible values of our decision variables). 
<br>&nbsp;&nbsp;In the spirit of transparency, I feel it's important for me to say that I haven't developed model of this nature in my professional career, as yet. But the understanding I do have is a result of the research I conducted at Healthscope, when this was discussed as a prospective project for Operations.
### Preprocessing steps
The same steps followed in the `src.preprocessing` module of the case study would also need to be followed for the proposed model as well:
- Dynamic imputation of missing values using MissForest
- Clean up the all the clinical charge columns using the `clean_clinical_charges()` function, such that the scientific notation is cleaned up, and the incredibly large values are limited to $1000 and a conversion from cents to dollars is also done. I'm not sure if my proposed `clean_clinical_charges()` function is actually correct? But, when I tested it against all the messy data in those clinical charges columns, it generated much more reasonable looking values. But, I'm happy to admit if my proposal is incorrect!
- Impute the columns (`UnplannedTheatreVisit`, `Readmission28Days`, and `PalliativeCareStatus`) that contain binary categoricals with a third category called 'unkown' (this would change the column to a categorical variable), since we there weren't sufficient instances of either true or false to infer dynamic imputation. Since there were so few values in these columns, any dynamic imputation technique that attempted a binary method would inevitably be deeply biased/flawed. So, I opted for creating this third category.
- Convert the `AdmissionTime` and `SeparationTime` columns to time format
- Create the objective function, `LengthOfStay`, by taking the difference between the `AdmissionDate` and `SeparationDate` columns.
### Evaluation metrics
- As far as my research has informed me (again, I'm happy to be proven wrong here, as I've only researched but not developed one of these models!), there's one primary evaluation metric used in Mathematical Optimisation - <a href='https://mobook.github.io/MO-book/notebooks/02/02-lad-regression.html'>Least Absolute Deviation Regression (LAD)</a>.
- The LAD Regression equation referenced in the above link refers to the following:<br>
<!-- Centered equation -->
$$
min \sum_{i=1}^n |e_i|
$$
<!-- Centered equation -->
Where:<br>
- $e_i=$ error term<br>
Another way to express the above that is a little more explainable:<br>
<!-- Centered equation -->
$$
S = \sum_{i=1}^n |y_i - f(x_i)|
$$
<!-- Centered equation -->
Where:<br>
- $y_i$ and $x_i$ are points in a data set
- $f(x_i)$ is a quadratic expression of type $f(x) = ax^2+bx+c$ where the parameters of $a,b$ and $c$ are not yet known
- $S=$ Sum of the absolute error values<br>
So the above equations (really it's just one equation that is slightly rewritten) refer to the minimisation of the error term when given a set of data points. This technique is also used as part of the $L_1$ <a href= 'https://en.wikipedia.org/wiki/Lp_space'>sum of the absolute errors</a>.

## Part 5: MLOps and Deployment
Once the model is developed in close conjunction with stakeholders, and has received the necessary sign offs to move into production, the following steps (in no particular order) would be one way to deploy the model (I've used Azure for production deployment at Healthscope, so I might use that as a guide in terms of terminology):
- Build the <b>ML Pipelines</b> in YAML that will connect each of the Python modules such that they run synchronously. For instance a pipeline (in synchronous order of execution may look like):
    1. The model may need to connect to the database to run a SQL query and grab the necessary data before anything else occurs. 
    2. Similarly there might external sources of data that have been identified whereby we may need to connect to an API and access that data. Steps 1 and 2 would run in parallel, but both must complete for step 3 to commence.
    3. Perform any necessary data transformations or preprocessing steps.
    4. If there are any parameter optimisation steps necessary run those after preprocessing has completed. As I understand it, in Mathematical Optimisation, parameters that determine index sets or upper/lower bounds can make up an immutable parameter space. Those could be set in this step.
    5. Run the optimisation models for every ward that is in-scope for the model.
    6. Connect to and run any front end applications that may need to be run, such that results of the models can be delivered to the stakeholders.
- <b>Set schedule YAML for the pipeline</b> to execute at a given frequency that is agreed upon by stakeholders (maybe once a week/fortnight? However, often is required)
- <b>Set up data drift</b>. This will also have an agreed upon frequency with stakeholders where it will run to determine if there's been any meaningful changes in the data distributions that can cause the model to underperform or even collapse altogether. For instance, if the model is set at the ward level, wards can temporarily shutdown or merge that can cause the measured patient activity to suddenly and unexpectedly hit 0. 
<br>&nbsp;&nbsp;<a href='https://devblogs.microsoft.com/ise/building-a-clinical-data-drift-monitoring-system-with-azure-devops-azure-databricks-and-mlflow/'> Microsoft</a> had developed some code that was purpose built for clinical applications and shared it on a public GitHub repo that had an implementation of data drift that utilised a 2-tailed Probability Distribution by way of the K-S (Kolmogorov–Smirnov) Test. If there was a statistically significant change between the 2 distributions (i.e. if p < 0.05 set as default). In order for this test to run it requires a reference period and current period to run. The reference period would be a preagreed upon offset to the date the data drift runs. Something like:<br>
`from datetime import datetime, timedelta`<br>
`now = datetime.now()`<br>
`reference_date = timedelta(days=14)`
<br>&nbsp;&nbsp;At Healthscope, if data drift was detected, an email with the ward ids would be sent by the Azure platform to the DS team. Upon which I would conduct an investigation as to what may have caused the data to drift. Almost all of the time, it was caused by wards temporarily shutting down or merging with a neighbouring ward.
- <b>Set up model drift</b>. Unlike data drift, which is empirically driven, model drift is sadly a much more nuanced and difficult thing to implement in a robust manner. Primarily because many of the causes of model drift may not be directly measurable until they occur, or we're not aware of the particulars of a model drift until after the fact!
<br>&nbsp;&nbsp;In saying the above, the following is a list of model drift types:
    - <b>Label drift</b>: This is a change when there's a change in the class label, or predictor (Y), itself. For example when I was referring to Healthscope's wards temporarily, but suddenly shutting down or merging, this would an example of Label drift, since it directly effects the predictor (Y).
    - <b>Feature drift</b>: When some (or all) of the features (X) that enrich the predictor (Y) change in some significant way. For example, the 2nd model I had developed for Healthscope was a short window (3 day) forecast of ED Presentations. In that model there were 5 features - from memory, I think they were transfer_reason, type, arrival, triage_category, and value. Please don't quote me on the exact list, but I think these were the additional features I used to increase the accuracy of the forecasts!
    <br>&nbsp;&nbsp;So in the case of the ED forecast model, if any or all of those features altered in some statistically significant way, it would account for feature drift, and will then become a driver in potentially decreasing the accuracy of the predictions (Y).
    - <b>Concept drift</b>: This refers to any latent, or unmeasured, variable that can strongly influence the predictor (Y). In the Healthscope ED model case as an example, if there's a nearby disaster, major traffic accident or anything of that like, it would directly influence the number of Presentations to any of the Healthscope's nearby EDs. However, that data is not part of my forecast model! As such, any such incidents of that nature would likely cause my model 'shock', and the accuracy would probably collapse.
<br><br>&nbsp;&nbsp;Depending on which literature you're reviewing, you'll tend to see differences in the above list. But from my research, professional experience, and understanding of model drift, the above list tends to be a reasonable coverage of the topic. But, as mentioned, capturing model drift is especially insidious in MLOps world. From my Healthscope experience, my approach to addressing model drift was to write and present to the Program Manager and Program Director a 12-page document that detailed all of the risks that were associated with deploying the patient activity forecast models into production. Those risks were an enumeration of the above topics around model drift!